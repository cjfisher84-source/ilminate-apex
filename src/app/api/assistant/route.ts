import { NextRequest, NextResponse } from 'next/server';
import { mockCyberScore, mockAIThreats, mockThreatFamilies } from '@/lib/mock';

/**
 * Security Assistant API
 * 
 * Provides intelligent responses to security queries by analyzing
 * real threat data from the dashboard.
 * 
 * Current: Deterministic mock responses (SSR-safe)
 * Future: Wire to AWS Lambda for live LLM analysis
 */
export async function POST(req: NextRequest) {
  try {
    const { prompt } = await req.json();

    // Get real data from mock functions (deterministic)
    const scoreValue = mockCyberScore();
    const score = {
      score: scoreValue,
      protectionRate: 94.2,
      responseTime: '2.3m',
      falsePositiveRate: '0.8%'
    };
    const aiThreats = mockAIThreats();
    const threatFamilies = mockThreatFamilies();
    
    // Analyze prompt intent
    const lower = String(prompt || '').toLowerCase();

    let reply = '';
    
    if (lower.includes('investigate') || lower.includes('threat')) {
      // Use real AI threats data
      const topThreat = aiThreats[0];
      const totalThreats = aiThreats.reduce((sum, t) => sum + t.count, 0);
      
      reply = `Top threat today:
‚Ä¢ Campaign: "${topThreat.type}"
‚Ä¢ Volume: ${topThreat.count} incidents detected
‚Ä¢ Total AI threats: ${totalThreats} across ${aiThreats.length} categories
‚Ä¢ Current Security Score: ${score.score}/100

Recommended actions:
1) Auto-quarantine similar messages (rule recommended)
2) Review ${topThreat.type} patterns for false positives
3) Block identified malicious domains
4) Notify security team of emerging threat pattern

Response Time: ${score.responseTime}
False Positive Rate: ${score.falsePositiveRate}`;
      
    } else if (lower.includes('posture') || lower.includes('improve')) {
      // Use real security score data
      const protectionGap = 100 - score.protectionRate;
      
      reply = `Posture improvements (prioritized):

Current Status:
‚Ä¢ Security Score: ${score.score}/100
‚Ä¢ Protection Rate: ${score.protectionRate}%
‚Ä¢ Response Time: ${score.responseTime}
‚Ä¢ False Positives: ${score.falsePositiveRate}

Top Recommendations:
1) DMARC enforcement: Move to p=reject policy (currently p=quarantine)
2) MFA coverage: Enable for remaining admin accounts
3) Response time: Target <2m (current: ${score.responseTime})
4) AI threat monitoring: Focus on ${aiThreats[0].type} (${aiThreats[0].count} recent)
5) Reduce false positives: Current ${score.falsePositiveRate} ‚Üí target <0.5%

Quick wins:
‚Ä¢ Enable additional email authentication checks
‚Ä¢ Update endpoint protection policies
‚Ä¢ Review and tune detection rules`;
      
    } else if (lower.includes('trend') || lower.includes('summary') || lower.includes('risk')) {
      // Use real threat family data
      const totalIncidents = threatFamilies.reduce((sum, t) => sum + t.count, 0);
      const highestThreat = threatFamilies[0];
      
      reply = `Risk trend (last 30 days):

Overall Metrics:
‚Ä¢ Security Score: ${score.score}/100 (${score.score > 80 ? 'Good' : 'Needs Improvement'})
‚Ä¢ Total incidents: ${totalIncidents}
‚Ä¢ Protection Rate: ${score.protectionRate}%
‚Ä¢ Mean time to respond: ${score.responseTime}
‚Ä¢ False positives: ${score.falsePositiveRate} (stable)

Threat Breakdown:
${threatFamilies.slice(0, 4).map(t => `‚Ä¢ ${t.name}: ${t.count} incidents (${t.trend})`).join('\n')}

Top Risk:
‚Ä¢ ${highestThreat.name}: ${highestThreat.count} incidents (${highestThreat.trend})

Notable insights:
‚Ä¢ AI-generated threats increasing across ${aiThreats.length} categories
‚Ä¢ ${aiThreats.reduce((sum, t) => sum + t.count, 0)} total AI-related incidents
‚Ä¢ Protection effectiveness: ${score.protectionRate}%

Outlook:
‚Ä¢ Continue monitoring ${highestThreat.name} patterns
‚Ä¢ DKIM/DMARC hardening recommended
‚Ä¢ Consider additional AI threat detection rules`;
      
    } else if (lower.includes('score') || lower.includes('status')) {
      reply = `Current Security Status:

üìä Cyber Security Score: ${score.score}/100
üõ°Ô∏è Protection Rate: ${score.protectionRate}%
‚ö° Response Time: ${score.responseTime}
‚úÖ False Positives: ${score.falsePositiveRate}

Recent Activity:
${aiThreats.slice(0, 3).map(t => `‚Ä¢ ${t.type}: ${t.count} incidents`).join('\n')}

Overall Health: ${score.score > 85 ? 'üü¢ Excellent' : score.score > 70 ? 'üü° Good' : 'üî¥ Needs Attention'}`;
      
    } else {
      // Fallback with contextual help
      reply = `I can help you with:

üîç **Threat Investigation**
‚Ä¢ "Investigate today's top threat"
‚Ä¢ "What's the highest risk right now?"
‚Ä¢ "Show me recent AI threats"

üõ°Ô∏è **Security Posture**
‚Ä¢ "How can I improve our security?"
‚Ä¢ "What's our current security score?"
‚Ä¢ "Recommend posture improvements"

üìä **Risk Analysis**
‚Ä¢ "Summarize 30-day risk trends"
‚Ä¢ "What's our protection rate?"
‚Ä¢ "Show me threat statistics"

Try asking: "What's our security score?" or use one of the quick action buttons above!`;
    }

    return NextResponse.json({ reply });
    
  } catch (e: any) {
    console.error('[ASSISTANT API ERROR]', e);
    return NextResponse.json({ 
      error: e?.message ?? 'error',
      reply: 'Sorry, I encountered an error processing your request. Please try again.'
    }, { status: 500 });
  }
}

